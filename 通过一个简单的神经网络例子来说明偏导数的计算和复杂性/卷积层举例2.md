我们将按照你提供的网络结构一步一步地计算这个简单卷积神经网络的前向传播和反向传播过程。

### 网络结构
1. **输入图像** \( X \)（尺寸为 \( 4 \times 4 \) ）：
   \[
   X = \begin{bmatrix}
   0.1 & 0.2 & 0.3 & 0.4 \\
   0.5 & 0.6 & 0.7 & 0.8 \\
   0.9 & 1.0 & 1.1 & 1.2 \\
   1.3 & 1.4 & 1.5 & 1.6
   \end{bmatrix}
   \]

2. **卷积核** \( W \)（尺寸为 \( 2 \times 2 \) ）：
   \[
   W = \begin{bmatrix}
   0.1 & 0.2 \\
   0.3 & 0.4
   \end{bmatrix}
   \]
   **偏置** \( b \)：
   \[
   b = 0.5
   \]

3. **池化层**：使用 \( 2 \times 2 \) 的最大池化层。

4. **全连接层**：1个输出神经元，激活函数为 Sigmoid。

5. **真实值** \( y_{true} = 1 \)。

### 前向传播

#### 1. 卷积层

我们将卷积核在输入图像上滑动，并计算输出特征图的每个元素。卷积操作可以表示为：
\[ 
y_{i,j} = \sum_{m=1}^{2} \sum_{n=1}^{2} W_{m,n} \times X_{i+m-1,j+n-1} + b 
\]

- **输出特征图**尺寸为 \( 3 \times 3 \)：

计算每个位置的卷积输出：

- \( y_{1,1} \):
  \[
  y_{1,1} = (0.1 \times 0.1) + (0.2 \times 0.2) + (0.3 \times 0.5) + (0.4 \times 0.6) + 0.5
  = 0.01 + 0.04 + 0.15 + 0.24 + 0.5 = 0.94
  \]
- \( y_{1,2} \):
  \[
  y_{1,2} = (0.1 \times 0.2) + (0.2 \times 0.3) + (0.3 \times 0.6) + (0.4 \times 0.7) + 0.5
  = 0.02 + 0.06 + 0.18 + 0.28 + 0.5 = 1.04
  \]
- \( y_{1,3} \):
  \[
  y_{1,3} = (0.1 \times 0.3) + (0.2 \times 0.4) + (0.3 \times 0.7) + (0.4 \times 0.8) + 0.5
  = 0.03 + 0.08 + 0.21 + 0.32 + 0.5 = 1.14
  \]
- \( y_{2,1} \):
  \[
  y_{2,1} = (0.1 \times 0.5) + (0.2 \times 0.6) + (0.3 \times 0.9) + (0.4 \times 1.0) + 0.5
  = 0.05 + 0.12 + 0.27 + 0.40 + 0.5 = 1.34
  \]
- \( y_{2,2} \):
  \[
  y_{2,2} = (0.1 \times 0.6) + (0.2 \times 0.7) + (0.3 \times 1.0) + (0.4 \times 1.1) + 0.5
  = 0.06 + 0.14 + 0.30 + 0.44 + 0.5 = 1.44
  \]
- \( y_{2,3} \):
  \[
  y_{2,3} = (0.1 \times 0.7) + (0.2 \times 0.8) + (0.3 \times 1.1) + (0.4 \times 1.2) + 0.5
  = 0.07 + 0.16 + 0.33 + 0.48 + 0.5 = 1.54
  \]
- \( y_{3,1} \):
  \[
  y_{3,1} = (0.1 \times 0.9) + (0.2 \times 1.0) + (0.3 \times 1.3) + (0.4 \times 1.4) + 0.5
  = 0.09 + 0.20 + 0.39 + 0.56 + 0.5 = 1.74
  \]
- \( y_{3,2} \):
  \[
  y_{3,2} = (0.1 \times 1.0) + (0.2 \times 1.1) + (0.3 \times 1.4) + (0.4 \times 1.5) + 0.5
  = 0.10 + 0.22 + 0.42 + 0.60 + 0.5 = 1.84
  \]
- \( y_{3,3} \):
  \[
  y_{3,3} = (0.1 \times 1.1) + (0.2 \times 1.2) + (0.3 \times 1.5) + (0.4 \times 1.6) + 0.5
  = 0.11 + 0.24 + 0.45 + 0.64 + 0.5 = 1.94
  \]

得到卷积层的输出特征图 \( Y \)：
\[
Y = \begin{bmatrix}
0.94 & 1.04 & 1.14 \\
1.34 & 1.44 & 1.54 \\
1.74 & 1.84 & 1.94
\end{bmatrix}
\]

#### 2. 池化层

使用 \( 2 \times 2 \) 的最大池化层，对卷积层输出进行池化：

- \( p_{1,1} \) 是 \( 2 \times 2 \) 区域 \( [0.94, 1.04; 1.34, 1.44] \) 的最大值，结果为 \( 1.44 \)。
- \( p_{1,2} \) 是 \( 2 \times 2 \) 区域 \( [1.04, 1.14; 1.44, 1.54] \) 的最大值，结果为 \( 1.54 \)。
- \( p_{2,1} \) 是 \( 2 \times 2 \) 区域 \( [1.34, 1.44; 1.74, 1.84] \) 的最大值，结果为 \( 1.84 \)。
- \( p_{2,2} \) 是 \( 2 \times 2 \) 区域 \( [1.44, 1.54; 1.84, 1.94] \) 的最大值，结果为 \( 1.94 \)。

池化层的输出 \( P \)：
\[
P = \begin{bmatrix}
1.44 & 1.54 \\
1.84 & 1.94
\end{bmatrix}
\]

#### 3. 全连接层

将池化层的输出 \( P \) 展平（Flatten）并输入到一个神经元进行计算：

输入向量 \( [1.44, 1.54, 1.84, 1.94] \) 与权重 \( w \) 相乘，并加上偏置：

假设全连接层的权重为 \( w = [0.1, 0.2, 0.3, 0.4] \)，偏置为 \( b_f = 0.5 \)，则输出 \( z \) 为：

\[
z = (1.44 \times 0.1) + (1.54 \times 0.2) + (1.84 \times 0.3) + (1.94 \times 0.4) + 0.5
\]
\[
z = 0.144 + 0.308 + 0.552 + 0.776 + 0.5 = 2.28
\]

通过 Sigmoid

我们将继续完成反向传播过程，计算损失函数相对于各个参数的偏导数。以下是详细的步骤。

### 1. 前向传播的最后一步：Sigmoid 激活函数

前面我们得到了全连接层的输出 \( z = 2.28 \)，现在将其输入 Sigmoid 激活函数：

\[
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-2.28}} \approx 0.907
\]

### 2. 计算损失函数

假设我们使用均方误差 (MSE) 作为损失函数：

\[
L = \frac{1}{2} (\hat{y} - y_{\text{true}})^2 = \frac{1}{2} (0.907 - 1)^2 \approx \frac{1}{2} \times 0.0087 = 0.00435
\]

### 3. 反向传播

现在我们计算损失函数对各个参数的偏导数。

#### 3.1. 对全连接层的权重和偏置的偏导数

首先计算损失对输出 \( \hat{y} \) 的偏导数：

\[
\frac{\partial L}{\partial \hat{y}} = \hat{y} - y_{\text{true}} = 0.907 - 1 = -0.093
\]

接着，计算损失对 \( z \) 的偏导数，考虑到 Sigmoid 的导数：

\[
\frac{\partial \hat{y}}{\partial z} = \hat{y}(1 - \hat{y}) = 0.907 \times (1 - 0.907) \approx 0.084
\]

\[
\frac{\partial L}{\partial z} = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial z} = -0.093 \times 0.084 \approx -0.007812
\]

然后，计算损失对全连接层权重 \( w \) 的偏导数：

\[
\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial z} \times \frac{\partial z}{\partial w_i} = \frac{\partial L}{\partial z} \times P_i
\]

所以：

\[
\frac{\partial L}{\partial w_1} = -0.007812 \times 1.44 \approx -0.01125
\]
\[
\frac{\partial L}{\partial w_2} = -0.007812 \times 1.54 \approx -0.01203
\]
\[
\frac{\partial L}{\partial w_3} = -0.007812 \times 1.84 \approx -0.01437
\]
\[
\frac{\partial L}{\partial w_4} = -0.007812 \times 1.94 \approx -0.01515
\]

对于全连接层的偏置 \( b_f \)：

\[
\frac{\partial L}{\partial b_f} = \frac{\partial L}{\partial z} = -0.007812
\]

#### 3.2. 对池化层输出的偏导数

接着反向传播到池化层，池化层没有参数，但是我们需要计算损失对池化层输出 \( P \) 的偏导数：

\[
\frac{\partial L}{\partial P_i} = \frac{\partial L}{\partial z} \times w_i
\]

所以：

\[
\frac{\partial L}{\partial P_1} = -0.007812 \times 0.1 \approx -0.0007812
\]
\[
\frac{\partial L}{\partial P_2} = -0.007812 \times 0.2 \approx -0.001562
\]
\[
\frac{\partial L}{\partial P_3} = -0.007812 \times 0.3 \approx -0.002343
\]
\[
\frac{\partial L}{\partial P_4} = -0.007812 \times 0.4 \approx -0.003125
\]

由于池化层采用最大池化，只有最大值位置的梯度会传递回去，所以对应的卷积层输出的偏导数为：

\[
\frac{\partial L}{\partial Y_{i,j}} = 
\begin{cases} 
\text{对应池化窗口内最大值位置的}\ \frac{\partial L}{\partial P_i}, & \text{如果 } Y_{i,j} 是最大值 \\
0, & \text{否则}
\end{cases}
\]

在这个例子中，梯度将传递到卷积层输出 \( Y \) 中最大值所在的元素。

#### 3.3. 对卷积核权重和偏置的偏导数

计算损失对卷积核权重 \( W \) 的偏导数。这里需要将池化层输出的梯度传播回卷积层，再计算卷积层的权重梯度。

以 \( \frac{\partial L}{\partial w_1} \) 为例：

\[
\frac{\partial L}{\partial w_1} = \sum_{\text{所有滑动窗口}} \left( \frac{\partial L}{\partial Y_{i,j}} \times X_{i,j} \right)
\]

具体计算时，要将上面的梯度计算结果代入，考虑到每个位置的输入值。

#### 3.4. 对卷积核偏置的偏导数

卷积核的偏置 \( b \) 的梯度为所有卷积输出梯度之和：

\[
\frac{\partial L}{\partial b} = \sum_{\text{所有滑动窗口}} \frac{\partial L}{\partial Y_{i,j}}
\]

### 总结
我们通过详细的计算，得出了损失函数对各个参数的偏导数。这个过程展示了反向传播的完整过程，并说明了如何逐层计算梯度，从最终的输出层反传到最初的卷积层。这个例子说明了反向传播在卷积神经网络中的应用，尤其是涉及多个参数和层次时的复杂性。