当然可以！我们用一个简单的多层神经网络的例子，来展示如何计算偏导数。假设这个网络有一个输入层、一个隐藏层和一个输出层。

### 示例：三层神经网络

#### 1. 网络结构

- **输入层**：2个输入 \( x_1 \) 和 \( x_2 \)
- **隐藏层**：2个神经元，激活函数为 Sigmoid
- **输出层**：1个神经元，激活函数为 Sigmoid
- **权重与偏置**：
  - 输入到隐藏层的权重：
    - \( w_{11} = 0.5 \)（连接 \( x_1 \) 到 隐藏层神经元1）
    - \( w_{12} = 0.4 \)（连接 \( x_2 \) 到 隐藏层神经元1）
    - \( w_{21} = 0.3 \)（连接 \( x_1 \) 到 隐藏层神经元2）
    - \( w_{22} = 0.2 \)（连接 \( x_2 \) 到 隐藏层神经元2）
  - 隐藏层到输出层的权重：
    - \( w_{h1} = 0.6 \)（连接 隐藏层神经元1 到 输出层）
    - \( w_{h2} = 0.5 \)（连接 隐藏层神经元2 到 输出层）
  - 偏置：
    - 隐藏层偏置 \( b_1 = 0.1 \)，\( b_2 = 0.1 \)
    - 输出层偏置 \( b_o = 0.2 \)

- **输入**：\( x_1 = 1 \)，\( x_2 = 2 \)
- **真实值**：\( y_{true} = 1 \)

#### 2. 前向传播

**隐藏层计算**:

1. **神经元1**:
   \[
   z_1 = w_{11} x_1 + w_{12} x_2 + b_1 = 0.5 \times 1 + 0.4 \times 2 + 0.1 = 1.4
   \]
   \[
   a_1 = f(z_1) = \sigma(1.4) = \frac{1}{1 + e^{-1.4}} \approx 0.802
   \]

2. **神经元2**:
   \[
   z_2 = w_{21} x_1 + w_{22} x_2 + b_2 = 0.3 \times 1 + 0.2 \times 2 + 0.1 = 0.9
   \]
   \[
   a_2 = f(z_2) = \sigma(0.9) = \frac{1}{1 + e^{-0.9}} \approx 0.710
   \]

**输出层计算**:

3. **输出神经元**:
   \[
   z_o = w_{h1} a_1 + w_{h2} a_2 + b_o = 0.6 \times 0.802 + 0.5 \times 0.710 + 0.2 \approx 0.858
   \]
   \[
   y = f(z_o) = \sigma(0.858) = \frac{1}{1 + e^{-0.858}} \approx 0.702
   \]

#### 3. 损失函数

使用均方误差作为损失函数：
\[
L = \frac{1}{2}(y_{true} - y)^2 = \frac{1}{2}(1 - 0.702)^2 \approx 0.045
\]

#### 4. 计算偏导数

我们现在计算损失函数 \( L \) 对各个权重和偏置的偏导数。

1. **计算损失函数对输出的偏导数**：
   \[
   \frac{\partial L}{\partial y} = -(y_{true} - y) = -(1 - 0.702) = -0.298
   \]

2. **计算输出层激活的导数**：
   \[
   f'(z_o) = y(1 - y) \approx 0.702(1 - 0.702) \approx 0.209
   \]

3. **输出层权重的偏导数**：
   - 对于 \( w_{h1} \)：
   \[
   \frac{\partial L}{\partial w_{h1}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_o} \cdot a_1 \approx -0.298 \cdot 0.209 \cdot 0.802 \approx -0.050
   \]

   - 对于 \( w_{h2} \)：
   \[
   \frac{\partial L}{\partial w_{h2}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_o} \cdot a_2 \approx -0.298 \cdot 0.209 \cdot 0.710 \approx -0.044
   \]

4. **输出层偏置的偏导数**：
   \[
   \frac{\partial L}{\partial b_o} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_o} \approx -0.298 \cdot 0.209 \approx -0.062
   \]

5. **隐藏层激活的导数**：
   - 对于 \( a_1 \) 和 \( a_2 \):
   \[
   f'(z_1) = a_1(1 - a_1) \approx 0.802(1 - 0.802) \approx 0.158
   \]
   \[
   f'(z_2) = a_2(1 - a_2) \approx 0.710(1 - 0.710) \approx 0.205
   \]

6. **计算隐藏层权重的偏导数**：
   - 对于 \( w_{11} \)：
   \[
   \frac{\partial L}{\partial w_{11}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_o} \cdot \frac{\partial z_o}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot x_1
   \]
   其中：
   \[
   \frac{\partial z_o}{\partial a_1} = w_{h1} \approx 0.6
   \]
   所以：
   \[
   \frac{\partial L}{\partial w_{11}} \approx -0.298 \cdot 0.209 \cdot 0.6 \cdot 0.158 \cdot 1 \approx -0.010
   \]

   - 对于 \( w_{12} \)：
   \[
   \frac{\partial L}{\partial w_{12}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_o} \cdot \frac{\partial z_o}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot x_2
   \]
   \[
   \frac{\partial L}{\partial w_{12}} \approx -0.298 \cdot 0.209 \cdot 0.6 \cdot 0.158 \cdot 2 \approx -0.020
   \]

   - 对于 \( w_{21} \) 和 \( w_{22} \) 的计算过程类似。

7. **隐藏层偏置的偏导数**：
   - 对于 \( b_1 \) 和 \( b_2 \)：
   \[
   \frac{\partial L}{\partial b_1} \approx \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z_o} \cdot \frac{\partial z_o}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \approx -0.298 \cdot 0.209 \cdot 0.6 \cdot 0.158 \approx -0.010
   \]

   - 对于 \( b_2 \) 的计算类似。

### 总结结果

- 对于输出层的权重：
  - \( \frac{\partial L}{\partial w_{h1}} \approx -0.050 \)
  - \( \frac{\partial L}{\partial w_{h2}} \approx -0.044 \)
- 对于输出层偏置：
  - \( \frac{\partial L}{\partial b_o} \approx -0.062 \)
- 对于隐藏层的权重：
  - \( \frac{\partial L}{\partial w_{11}} \approx -0.010 \)
  - \( \frac{\partial L}{\partial w_{12}} \approx -0.020 \)
  - \( \frac{\partial L}{\partial w_{21}} \) 和 \( \frac{\partial L}{\partial w_{22}} \) 的计算过程类似。

### 解释复杂性

在这个多层神经网络的例子中，我们可以看到：

1. **链式法则的应用**：每一层的导数都依赖于前一层的结果，导致计算过程复杂。
2. **激活函数的非线性特性**：Sigmoid 函数的导数需要计算，增加了额外的步骤。
3. **参数数量的增加**：每一层的权重和偏置数量增加，导致需要计算的偏导数数量显著增加。

这个简单的多层神经网络例子展示了如何进行前向传播和反向传播，以及如何计算损失函数对各个参数的偏导数。