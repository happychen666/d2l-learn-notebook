这个理论探讨了神经网络中交叉熵损失函数的定义及其在特定场景下的应用，特别是当输出层的激活函数为 sigmoid 时的情况。

### 交叉熵损失函数的定义

交叉熵损失函数通常用于二分类任务，它的数学表达式为：

\[
\text{交叉熵} = -\sum_{i} p_i \ln(q_i)
\]

其中：

- \(p_i\) 是真实标签的概率分布。
- \(q_i\) 是模型预测的概率分布。

对于二分类问题，\(p_i\) 通常是 \(\{0, 1\}\) 的取值。

### Sigmoid 激活函数

在神经网络中，如果输出层使用的是 sigmoid 激活函数，则输出可以解释为一种概率，即神经元的输出值 \(a^l_j\) 是在区间 [0, 1] 之间的。这意味着我们可以将其解释为某个类别的概率。

### 交叉熵与 Sigmoid 输出的结合

文中提到，如果神经元的激活值 \(a\) 可以看作是一个概率分布，那么交叉熵损失函数的定义就可以联系到神经元的激活值和其补值 \(1-a\)。然而，当输出层有多个 sigmoid 神经元时，激活值 \(a^l_j\) 并不总能形成概率分布，因此在这种情况下，简单地将交叉熵定义为 \(\sum p_i \ln q_j\) 并没有实际意义，因为它并没有用到概率分布。

### 对策

相反，可以将公式应用于每个神经元的交叉熵求和，即对每个单独神经元的激活值计算交叉熵损失函数。这相当于将多个二元分类问题组合成一个多分类问题。

### 数值举例说明

假设你有一个神经网络模型用于二分类任务，输出层的激活函数是 sigmoid。假设真实标签 \(p = 1\)（表示正类），模型的预测输出 \(q = 0.8\)。

交叉熵损失函数的计算为：

\[
\text{交叉熵} = -[p \ln(q) + (1 - p) \ln(1 - q)]
\]

代入数值：

\[
\text{交叉熵} = -[1 \cdot \ln(0.8) + 0 \cdot \ln(0.2)] = -\ln(0.8)
\]

计算得出：

\[
\text{交叉熵} \approx -(-0.2231) = 0.2231
\]

这表示损失为 0.2231。

如果输出层有多个 sigmoid 神经元，那么你会对每个神经元单独计算交叉熵，然后求和，这就解释了为什么交叉熵可以分解成每个神经元独立的交叉熵求和的形式。

这个数值例子说明了如何使用交叉熵损失函数计算模型预测值与真实值之间的差异。
