好的，我们可以通过一个简单的数值示例来说明 Sigmoid 激活函数导致的梯度消失问题。

### Sigmoid 激活函数定义

Sigmoid 函数的定义为：
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
它的导数为：
\[
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
\]

### 示例

假设我们有一个简单的神经网络，只有一层，输入 \( x \) 为 -10、0 和 10，分别计算 Sigmoid 函数的输出和导数。

1. **输入 \( x = -10 \)**：
   - \(\sigma(-10) = \frac{1}{1 + e^{10}} \approx 0.0000454\)
   - \(\sigma'(-10) \approx 0.0000454 \cdot (1 - 0.0000454) \approx 0.0000454\) （非常小）

2. **输入 \( x = 0 \)**：
   - \(\sigma(0) = \frac{1}{1 + e^{0}} = 0.5\)
   - \(\sigma'(0) = 0.5 \cdot (1 - 0.5) = 0.25\) （相对较大）

3. **输入 \( x = 10 \)**：
   - \(\sigma(10) = \frac{1}{1 + e^{-10}} \approx 0.9999546\)
   - \(\sigma'(10) \approx 0.9999546 \cdot (1 - 0.9999546) \approx 0.0000454\) （非常小）

### 梯度消失的影响

- 当输入为 -10 或 10 时，Sigmoid 的导数接近于 0，这会导致在反向传播过程中，经过这些层的梯度几乎为零。
- 假设在某一层的输出经过 Sigmoid 激活后，梯度为 \( \frac{\partial L}{\partial a} \)（损失对激活值的导数），那么在反向传播时：
\[
\frac{\partial L}{\partial z} = \frac{\partial L}{\partial a} \cdot \sigma'(z)
\]
如果 \( \sigma'(z) \) 非常小，最终的梯度 \( \frac{\partial L}{\partial z} \) 也将非常小，导致参数更新几乎停止。

### 总结

通过这个简单的数值例子，可以看到，当输入值远离0时，Sigmoid 函数的导数变得非常小，从而导致梯度消失。这是深层神经网络训练中的一个重要问题。
 