在神经网络中，反向传播（Backpropagation）是用于训练模型的关键算法，它通过计算损失函数关于网络参数的梯度来更新模型的权重和偏置。具体来说，参数更新主要指以下内容：

### 1. 权重（Weights）

神经网络中的每一层都有一组权重，用于将输入信号转换为输出信号。在反向传播中，我们会计算每个权重对损失函数的梯度，并根据该梯度调整权重。权重更新的公式通常是：
\[
w = w - \eta \cdot \frac{\partial L}{\partial w}
\]
其中：

- \(w\) 是权重
- \(\eta\) 是学习率（一个超参数，控制更新步长）
- \(\frac{\partial L}{\partial w}\) 是损失函数 \(L\) 相对于权重 \(w\) 的导数

### 2. 偏置（Biases）

每一层的输出除了依赖于权重外，还会有一个偏置项。偏置的作用是使得模型更加灵活。偏置的更新也类似于权重的更新：
\[
b = b - \eta \cdot \frac{\partial L}{\partial b}
\]
其中：

- \(b\) 是偏置
- \(\frac{\partial L}{\partial b}\) 是损失函数 \(L\) 相对于偏置 \(b\) 的导数

### 3. 其他参数

在某些情况下，神经网络可能还包含其他类型的可训练参数，例如：

- **卷积核**：在卷积神经网络中，卷积核（滤波器）也是需要更新的参数。
- **批归一化参数**：在使用批归一化（Batch Normalization）时，可能需要更新的参数包括缩放因子和偏移量。
- **权重衰减参数**：如果使用了正则化方法（如 L2 正则化），可能还需要更新相关的正则化参数。

### 反向传播的步骤

反向传播的基本步骤如下：

1. **前向传播**：计算网络的输出和损失。
2. **计算损失的梯度**：从损失函数出发，计算每一层的输出对损失的贡献。
3. **反向传播梯度**：将梯度从输出层向输入层反向传播，计算每一层的权重和偏置的梯度。
4. **更新参数**：使用计算得到的梯度更新权重和偏置。

### 总结

反向传播中的参数更新主要指权重和偏置的调整，这些参数的更新通过计算损失函数的梯度来实现，以使模型更好地拟合训练数据。
