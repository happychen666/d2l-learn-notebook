“网络实际上是在学习输入 \( x \) 与目标输出之间的‘残差’，而不是直接学习从输入到输出的映射” 这一句话的意思是，在ResNet（残差网络）中，模型的目标不是直接找出输入 \( x \) 到输出 \( y \) 的转换关系，而是要学习一个新的函数 \( F(x) \)，使得：

\[ y = F(x) + x \]

这里的 \( y \) 是目标输出，而 \( F(x) \) 是残差部分。具体来说：

1. **直接学习映射**：
   - 在传统的神经网络中，模型会被训练来直接学习从输入 \( x \) 到输出 \( y \) 的关系。如果我们把这个关系称为 \( H(x) \)，那么模型试图找到 \( H(x) = y \)。

2. **学习残差**：
   - 在ResNet中，模型被设计成学习输入 \( x \) 和目标输出 \( y \) 之间的差异，称为“残差”或“偏差”。所以模型实际上学习的是 \( F(x) = y - x \)，即 \( F(x) \) 是输入和输出之间的差值。
   - 因此，ResNet的输出为 \( y = F(x) + x \)。这意味着网络只需学习如何在输入的基础上进行小的调整（残差），而不是从头开始学习整个映射。

### 直观理解

1. **简化学习过程**：
   - 如果目标输出 \( y \) 与输入 \( x \) 非常接近，学习残差 \( F(x) \) 会比学习整个映射 \( H(x) \) 更容易，因为残差通常是一个相对较小的值。

2. **避免梯度消失**：
   - 在深度网络中，随着层数的增加，信号在传播过程中可能会变得非常微弱（梯度消失）。通过学习残差，信息可以通过短路连接直接传递，从而减轻这一问题。

### 例子

假设我们的输入是图像 \( x \)，而目标输出是对这个图像的分类 \( y \)。

- 在传统网络中，网络需要学习如何将 \( x \) 转换为 \( y \)。
- 在ResNet中，网络只需学习 \( F(x) \)，即如何从 \( x \) 调整到 \( y \)，比如可能只需增加一个小的偏移量。

这种方法使得网络能够更轻松地学习到复杂的特征，并且在训练深层网络时表现得更稳定。